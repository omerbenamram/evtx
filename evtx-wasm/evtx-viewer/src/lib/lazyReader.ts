import type { EvtxRecord, ParseResult } from "./types";
import { logger } from "./logger";

// Static type-only import of the wasm-bindgen generated module. This file is
// generated by `wasm-pack` into `src/wasm/evtx_wasm.d.ts` via the dev script.
// At runtime we will still use the JS glue file but we cast it to this type so
// TypeScript recognises the methods (e.g., `parse_chunk_records`).
type WasmBindings = typeof import("../wasm/evtx_wasm");

export interface ChunkWindow {
  chunkIndex: number;
  start: number; // record offset within chunk
  limit: number; // max records to retrieve (<= 0 means no limit)
}

export class LazyEvtxReader {
  private wasmModule: WasmBindings | null = null;
  private parser!: InstanceType<WasmBindings["EvtxWasmParser"]>;
  private cache: Map<string, EvtxRecord[]> = new Map();

  private async init(data: Uint8Array): Promise<void> {
    if (!this.wasmModule) {
      // Dynamically load the JS glue file and cast to the static typings.
      this.wasmModule = (await import(
        "../wasm/evtx_wasm.js"
      )) as unknown as WasmBindings;
    }
    this.parser = new this.wasmModule.EvtxWasmParser(data);
  }

  /** Factory helper that reads a File into memory and constructs a reader. */
  static async fromFile(file: File): Promise<LazyEvtxReader> {
    const buf = new Uint8Array(await file.arrayBuffer());
    const reader = new LazyEvtxReader();
    await reader.init(buf);
    return reader;
  }

  private cacheKey(chunk: number, start: number, limit: number): string {
    return `${chunk}-${start}-${limit}`;
  }

  /**
   * Recursively convert Map instances (returned by `serde_wasm_bindgen`) into
   * plain JavaScript objects so consumers can use regular property access.
   */
  private static mapToObject(input: unknown): unknown {
    if (input instanceof Map) {
      const out: Record<string, unknown> = {};
      input.forEach((v, k) => {
        out[k as string] = LazyEvtxReader.mapToObject(v);
      });
      return out;
    }
    if (Array.isArray(input))
      return input.map((v) => LazyEvtxReader.mapToObject(v));
    return input;
  }

  /** Retrieve records for the given window (cached). */
  async getWindow(win: ChunkWindow): Promise<EvtxRecord[]> {
    const key = this.cacheKey(win.chunkIndex, win.start, win.limit);
    const cached = this.cache.get(key);
    if (cached) return cached;

    const res: ParseResult = this.parser.parse_chunk_records(
      win.chunkIndex,
      win.start,
      win.limit > 0 ? win.limit : undefined
    ) as unknown as ParseResult;

    // Convert each record from potential Map/Array hybrids into plain objects.
    const records = (res.records as unknown[]).map((r) =>
      LazyEvtxReader.mapToObject(r)
    ) as EvtxRecord[];

    logger.debug(`LazyEvtxReader fetched ${records.length} recs`, {
      window: win,
    });

    this.cache.set(key, records);
    return records;
  }

  /** Retrieve high-level information about the EVTX file (chunk counts, etc.). */
  async getFileInfo(): Promise<{
    totalChunks: number;
    chunkRecordCounts: number[];
  }> {
    if (!this.parser) {
      throw new Error("Parser not initialised â€“ call fromFile() first");
    }

    // The Rust binding returns `FileInfo` with snake_case keys; we only need
    // `total_chunks` and the per-chunk `record_count` values to compute
    // record offsets.  We decode and massage them into simple JS primitives.
    // eslint-disable-next-line @typescript-eslint/no-unsafe-assignment
    const raw = this.parser.get_file_info() as unknown as {
      total_chunks: number;
      chunks: { record_count: string }[];
    };

    const MAX_SAFE = BigInt(Number.MAX_SAFE_INTEGER);
    const chunkRecordCounts = raw.chunks.map((c, idx) => {
      try {
        const big = BigInt(c.record_count);
        if (big <= MAX_SAFE) return Number(big);

        logger.info("Chunk record_count exceeds 53-bit, clamping", {
          idx,
          raw: c.record_count,
        });
        return Number.MAX_SAFE_INTEGER; // upper bound so downstream maths stays finite
      } catch {
        logger.error("Failed to parse chunk record_count as BigInt", {
          idx,
          raw: c.record_count,
        });
        return 0;
      }
    });
    return {
      totalChunks: raw.total_chunks,
      chunkRecordCounts,
    };
  }
}
